## Deep Dive: External Service Integrations & Emotional Tone Analysis

I'll provide detailed technical implementations for both components, focusing on practical integration and advanced analysis.

## Chunk 6.1: External Service Integration Framework

```python
# Unified External Service Integration System
import asyncio
import aiohttp
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from enum import Enum

class ServiceType(Enum):
    GMAIL = "gmail"
    GOOGLE_PHOTOS = "google_photos"
    GOOGLE_DRIVE = "google_drive"
    FACEBOOK = "facebook"
    TWITTER = "twitter"
    LINKEDIN = "linkedin"
    CALENDAR = "calendar"
    CONTACTS = "contacts"

@dataclass
class ServiceCredentials:
    access_token: str
    refresh_token: str
    token_expiry: int
    scopes: List[str]

@dataclass
class EnrichmentResult:
    service: ServiceType
    content_type: str
    content: Dict[str, Any]
    confidence: float
    source_id: str
    timestamp: str
    relevance_score: float

class ExternalServiceManager:
    def __init__(self, encryption_key: str):
        self.encryption_key = encryption_key
        self.service_clients = {}
        self.rate_limits = {}
        self.connected_services = set()
        
        # Initialize service-specific configurations
        self.service_configs = {
            ServiceType.GMAIL: {
                'base_url': 'https://gmail.googleapis.com/gmail/v1',
                'scopes': ['https://www.googleapis.com/auth/gmail.readonly'],
                'rate_limit': 1000,  # requests per 100 seconds
                'batch_size': 100
            },
            ServiceType.GOOGLE_PHOTOS: {
                'base_url': 'https://photoslibrary.googleapis.com/v1',
                'scopes': ['https://www.googleapis.com/auth/photoslibrary.readonly'],
                'rate_limit': 10000,
                'batch_size': 50
            },
            ServiceType.FACEBOOK: {
                'base_url': 'https://graph.facebook.com/v18.0',
                'scopes': ['email', 'user_photos', 'user_posts'],
                'rate_limit': 200,
                'batch_size': 25
            }
        }
        
        self.initialize_service_clients()

    def initialize_service_clients(self):
        """Initialize HTTP clients for each service with proper headers"""
        for service_type, config in self.service_configs.items():
            self.service_clients[service_type] = aiohttp.ClientSession(
                base_url=config['base_url'],
                headers={
                    'User-Agent': 'NIRVANA-Voice-Profiler/1.0',
                    'Accept': 'application/json'
                },
                timeout=aiohttp.ClientTimeout(total=30)
            )

    async def enrich_profile_from_services(self, profile_id: str, profile_data: Dict) -> List[EnrichmentResult]:
        """
        Main enrichment pipeline - coordinates enrichment from all connected services
        """
        enrichment_tasks = []
        results = []
        
        # Determine which services to query based on available data
        target_services = self._determine_relevant_services(profile_data)
        
        for service_type in target_services:
            if service_type in self.connected_services:
                task = asyncio.create_task(
                    self._enrich_from_service(service_type, profile_id, profile_data)
                )
                enrichment_tasks.append(task)
        
        # Execute all enrichment tasks concurrently
        if enrichment_tasks:
            service_results = await asyncio.gather(*enrichment_tasks, return_exceptions=True)
            
            for result in service_results:
                if isinstance(result, Exception):
                    print(f"Enrichment error: {result}")
                    continue
                results.extend(result)
        
        # Score and filter results by relevance
        scored_results = self._score_enrichment_relevance(results, profile_data)
        return scored_results

    def _determine_relevant_services(self, profile_data: Dict) -> List[ServiceType]:
        """Determine which services are most relevant for this profile"""
        services = []
        name = profile_data.get('display_name', '').lower()
        
        # Always check basic services
        services.extend([ServiceType.GMAIL, ServiceType.CONTACTS])
        
        # Add services based on relationship type
        relationship = profile_data.get('relationship_type', 'unknown')
        if relationship in ['family', 'friend']:
            services.extend([ServiceType.FACEBOOK, ServiceType.GOOGLE_PHOTOS])
        elif relationship in ['colleague', 'work']:
            services.extend([ServiceType.LINKEDIN, ServiceType.CALENDAR])
        
        # Add services based on available data
        if profile_data.get('email'):
            services.append(ServiceType.GMAIL)
        if profile_data.get('phone'):
            services.append(ServiceType.CONTACTS)
        
        return list(set(services))  # Remove duplicates

    async def _enrich_from_service(self, service_type: ServiceType, profile_id: str, 
                                 profile_data: Dict) -> List[EnrichmentResult]:
        """Service-specific enrichment implementation"""
        try:
            if service_type == ServiceType.GMAIL:
                return await self._enrich_from_gmail(profile_id, profile_data)
            elif service_type == ServiceType.GOOGLE_PHOTOS:
                return await self._enrich_from_google_photos(profile_id, profile_data)
            elif service_type == ServiceType.FACEBOOK:
                return await self._enrich_from_facebook(profile_id, profile_data)
            elif service_type == ServiceType.CONTACTS:
                return await self._enrich_from_contacts(profile_id, profile_data)
            elif service_type == ServiceType.CALENDAR:
                return await self._enrich_from_calendar(profile_id, profile_data)
        except Exception as e:
            print(f"Service {service_type} enrichment failed: {e}")
            return []

    async def _enrich_from_gmail(self, profile_id: str, profile_data: Dict) -> List[EnrichmentResult]:
        """Enrich profile from Gmail messages and contacts"""
        results = []
        name = profile_data['display_name']
        
        # Search for emails from/to this person
        query = f'from:"{name}" OR to:"{name}" OR subject:"{name}"'
        
        async with self.service_clients[ServiceType.GMAIL] as client:
            # Get messages
            messages_response = await client.get('/users/me/messages', params={
                'q': query,
                'maxResults': 50
            })
            
            if messages_response.status == 200:
                messages_data = await messages_response.json()
                
                for message in messages_data.get('messages', [])[:10]:  # Limit to 10 messages
                    # Get full message details
                    msg_detail = await client.get(f'/users/me/messages/{message["id"]}')
                    if msg_detail.status == 200:
                        msg_data = await msg_detail.json()
                        
                        # Extract message content
                        enrichment = self._parse_gmail_message(msg_data, name)
                        if enrichment:
                            results.append(EnrichmentResult(
                                service=ServiceType.GMAIL,
                                content_type='email',
                                content=enrichment,
                                confidence=0.85,
                                source_id=message['id'],
                                timestamp=enrichment.get('date', ''),
                                relevance_score=0.0  # Will be scored later
                            ))
        
        return results

    def _parse_gmail_message(self, message_data: Dict, target_name: str) -> Optional[Dict]:
        """Parse Gmail message for relevant enrichment data"""
        try:
            headers = {h['name']: h['value'] for h in message_data.get('payload', {}).get('headers', [])}
            
            # Extract body content
            body = self._extract_email_body(message_data.get('payload', {}))
            
            return {
                'subject': headers.get('Subject', ''),
                'from': headers.get('From', ''),
                'to': headers.get('To', ''),
                'date': headers.get('Date', ''),
                'snippet': message_data.get('snippet', ''),
                'body_preview': body[:500] if body else '',
                'labels': message_data.get('labelIds', [])
            }
        except Exception as e:
            print(f"Failed to parse Gmail message: {e}")
            return None

    async def _enrich_from_google_photos(self, profile_id: str, profile_data: Dict) -> List[EnrichmentResult]:
        """Enrich profile from Google Photos"""
        results = []
        name = profile_data['display_name']
        
        async with self.service_clients[ServiceType.GOOGLE_PHOTOS] as client:
            # Search for photos by person's name
            search_body = {
                'pageSize': 20,
                'filters': {
                    'contentFilter': {
                        'includedContentCategories': ['PEOPLE']
                    }
                }
            }
            
            # Note: Google Photos API doesn't support direct name search in basic API
            # This would require face grouping and recognition which is more advanced
            # For now, we'll search by date and other metadata
            
            photos_response = await client.post('/mediaItems:search', json=search_body)
            if photos_response.status == 200:
                photos_data = await photos_response.json()
                
                for photo in photos_data.get('mediaItems', [])[:10]:
                    # Analyze photo metadata for relevance
                    if self._is_photo_relevant(photo, name):
                        results.append(EnrichmentResult(
                            service=ServiceType.GOOGLE_PHOTOS,
                            content_type='photo',
                            content={
                                'url': photo.get('baseUrl', ''),
                                'filename': photo.get('filename', ''),
                                'mimeType': photo.get('mimeType', ''),
                                'metadata': photo.get('mediaMetadata', {}),
                                'description': photo.get('description', '')
                            },
                            confidence=0.7,
                            source_id=photo['id'],
                            timestamp=photo.get('mediaMetadata', {}).get('creationTime', ''),
                            relevance_score=0.0
                        ))
        
        return results

    async def _enrich_from_facebook(self, profile_id: str, profile_data: Dict) -> List[EnrichmentResult]:
        """Enrich profile from Facebook data"""
        results = []
        name = profile_data['display_name']
        
        async with self.service_clients[ServiceType.FACEBOOK] as client:
            # Search for person in friends
            friends_response = await client.get('/me/friends', params={
                'fields': 'id,name,picture,link',
                'limit': 100
            })
            
            if friends_response.status == 200:
                friends_data = await friends_response.json()
                
                for friend in friends_data.get('data', []):
                    if name.lower() in friend.get('name', '').lower():
                        # Found matching friend
                        results.append(EnrichmentResult(
                            service=ServiceType.FACEBOOK,
                            content_type='profile',
                            content={
                                'name': friend.get('name', ''),
                                'picture': friend.get('picture', {}).get('data', {}).get('url', ''),
                                'profile_url': friend.get('link', ''),
                                'facebook_id': friend.get('id', '')
                            },
                            confidence=0.9,
                            source_id=friend['id'],
                            timestamp='',  # Facebook doesn't provide friend date easily
                            relevance_score=0.0
                        ))
                        
                        # Get recent posts/interactions
                        posts = await self._get_facebook_posts(client, friend['id'])
                        results.extend(posts)
        
        return results

    async def _enrich_from_contacts(self, profile_id: str, profile_data: Dict) -> List[EnrichmentResult]:
        """Enrich profile from device/Google contacts"""
        results = []
        name = profile_data['display_name']
        
        # This would integrate with the device's contact API or Google People API
        # For demonstration, we'll simulate contact matching
        
        # Simulated contact data structure
        simulated_contacts = [
            {
                'name': name,
                'emails': ['chris@example.com'],
                'phones': ['+1234567890'],
                'organization': 'Tech Corp',
                'notes': 'Met at conference 2024'
            }
        ]
        
        for contact in simulated_contacts:
            if name.lower() in contact['name'].lower():
                results.append(EnrichmentResult(
                    service=ServiceType.CONTACTS,
                    content_type='contact',
                    content=contact,
                    confidence=0.95,
                    source_id=f"contact_{hash(name)}",
                    timestamp='',
                    relevance_score=0.8
                ))
        
        return results

    def _score_enrichment_relevance(self, results: List[EnrichmentResult], 
                                  profile_data: Dict) -> List[EnrichmentResult]:
        """Score and filter enrichment results by relevance to the profile"""
        for result in results:
            score = 0.0
            
            # Base score from confidence
            score += result.confidence * 0.3
            
            # Recency bonus (if timestamp available)
            if result.timestamp:
                recency = self._calculate_recency_score(result.timestamp)
                score += recency * 0.2
            
            # Content type weighting
            content_weights = {
                'contact': 0.3,
                'profile': 0.25,
                'email': 0.2,
                'photo': 0.15,
                'post': 0.1
            }
            score += content_weights.get(result.content_type, 0.1) * 0.3
            
            # Name matching bonus
            name = profile_data.get('display_name', '').lower()
            if any(name in str(value).lower() for value in result.content.values()):
                score += 0.2
            
            result.relevance_score = min(score, 1.0)  # Cap at 1.0
        
        # Sort by relevance and return top results
        return sorted(results, key=lambda x: x.relevance_score, reverse=True)[:15]

    def _calculate_recency_score(self, timestamp: str) -> float:
        """Calculate recency score (0-1) based on how recent the data is"""
        try:
            from datetime import datetime, timezone
            now = datetime.now(timezone.utc)
            data_time = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
            days_ago = (now - data_time).days
            
            if days_ago < 7: return 1.0
            elif days_ago < 30: return 0.7
            elif days_ago < 90: return 0.4
            elif days_ago < 365: return 0.2
            else: return 0.1
        except:
            return 0.1
```

## Chunk 6.2: Advanced Emotional Tone Analysis

```python
# Advanced Emotional Tone Analysis System
import numpy as np
from typing import Dict, List, Tuple
from dataclasses import dataclass
from enum import Enum
import librosa
import torch
import torch.nn as nn

class EmotionCategory(Enum):
    HAPPY = "happy"
    SAD = "sad" 
    ANGRY = "angry"
    FEARFUL = "fearful"
    SURPRISED = "surprised"
    DISGUSTED = "disgusted"
    NEUTRAL = "neutral"
    EXCITED = "excited"
    CALM = "calm"
    FRUSTRATED = "frustrated"

@dataclass
class EmotionalAnalysis:
    primary_emotion: EmotionCategory
    secondary_emotion: EmotionCategory
    confidence: float
    intensity: float  # 0-1 scale
    valence: float    # -1 (negative) to 1 (positive)
    arousal: float    # 0 (calm) to 1 (excited)
    emotion_scores: Dict[EmotionCategory, float]
    vocal_features: Dict[str, float]
    context_influence: float  # How much context affects the emotion

class EmotionalToneAnalyzer:
    def __init__(self, model_path: str = None):
        self.feature_extractor = VocalFeatureExtractor()
        self.emotion_model = self._load_emotion_model(model_path)
        self.context_analyzer = ContextAwareEmotionAnalyzer()
        
        # Emotion mapping based on vocal features research
        self.emotion_characteristics = {
            EmotionCategory.HAPPY: {
                'pitch_range': (1.2, 2.0),
                'speech_rate': (1.1, 1.8),
                'energy': (0.7, 1.0),
                'valence': (0.6, 1.0),
                'arousal': (0.6, 1.0)
            },
            EmotionCategory.SAD: {
                'pitch_range': (0.7, 1.0),
                'speech_rate': (0.6, 0.9),
                'energy': (0.3, 0.6),
                'valence': (-1.0, -0.3),
                'arousal': (0.1, 0.4)
            },
            EmotionCategory.ANGRY: {
                'pitch_range': (1.5, 2.5),
                'speech_rate': (1.2, 2.0),
                'energy': (0.8, 1.0),
                'valence': (-1.0, -0.5),
                'arousal': (0.8, 1.0)
            },
            EmotionCategory.NEUTRAL: {
                'pitch_range': (0.9, 1.1),
                'speech_rate': (0.9, 1.1),
                'energy': (0.4, 0.6),
                'valence': (-0.2, 0.2),
                'arousal': (0.3, 0.5)
            }
        }

    def analyze_emotion(self, audio_data: np.ndarray, 
                       sample_rate: int = 16000,
                       context: Dict = None) -> EmotionalAnalysis:
        """
        Comprehensive emotional analysis from audio data
        """
        # Extract vocal features
        features = self.feature_extractor.extract_comprehensive_features(audio_data, sample_rate)
        
        # Get base emotion prediction
        base_emotion, emotion_scores = self._predict_base_emotion(features)
        
        # Apply context-aware adjustment
        adjusted_emotion = self.context_analyzer.adjust_emotion_with_context(
            base_emotion, emotion_scores, context
        )
        
        # Calculate emotional dimensions
        valence, arousal = self._calculate_emotional_dimensions(features)
        intensity = self._calculate_emotional_intensity(features)
        
        return EmotionalAnalysis(
            primary_emotion=adjusted_emotion.primary,
            secondary_emotion=adjusted_emotion.secondary,
            confidence=adjusted_emotion.confidence,
            intensity=intensity,
            valence=valence,
            arousal=arousal,
            emotion_scores=emotion_scores,
            vocal_features=features,
            context_influence=adjusted_emotion.context_influence
        )

    def _predict_base_emotion(self, features: Dict) -> Tuple[EmotionCategory, Dict]:
        """Predict emotion using a combination of rule-based and model-based approaches"""
        # Rule-based scoring based on vocal characteristics
        rule_scores = self._rule_based_emotion_scoring(features)
        
        # Model-based prediction (if model available)
        if self.emotion_model:
            model_scores = self._model_based_prediction(features)
            # Combine rule-based and model-based scores
            combined_scores = self._combine_emotion_scores(rule_scores, model_scores)
        else:
            combined_scores = rule_scores
        
        # Get primary and secondary emotions
        emotions_sorted = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)
        primary_emotion = emotions_sorted[0][0]
        secondary_emotion = emotions_sorted[1][0] if len(emotions_sorted) > 1 else primary_emotion
        
        return primary_emotion, combined_scores

    def _rule_based_emotion_scoring(self, features: Dict) -> Dict[EmotionCategory, float]:
        """Rule-based emotion scoring using known vocal characteristics"""
        scores = {emotion: 0.0 for emotion in EmotionCategory}
        
        for emotion, characteristics in self.emotion_characteristics.items():
            score = 1.0
            
            # Check each characteristic and adjust score
            for char_name, (min_val, max_val) in characteristics.items():
                if char_name in features:
                    feature_val = features[char_name]
                    # Calculate how well feature matches expected range
                    if min_val <= feature_val <= max_val:
                        # Perfect match
                        char_score = 1.0
                    else:
                        # Distance from ideal range
                        distance = min(abs(feature_val - min_val), abs(feature_val - max_val))
                        char_score = max(0, 1 - distance)
                    
                    score *= char_score
            
            scores[emotion] = score
        
        # Normalize scores
        total = sum(scores.values())
        if total > 0:
            scores = {k: v/total for k, v in scores.items()}
        
        return scores

class VocalFeatureExtractor:
    """Advanced vocal feature extraction for emotion analysis"""
    
    def extract_comprehensive_features(self, audio_data: np.ndarray, 
                                     sample_rate: int) -> Dict[str, float]:
        """Extract comprehensive set of vocal features for emotion analysis"""
        features = {}
        
        # Basic audio features
        features.update(self._extract_basic_features(audio_data, sample_rate))
        
        # Spectral features
        features.update(self._extract_spectral_features(audio_data, sample_rate))
        
        # Prosodic features
        features.update(self._extract_prosodic_features(audio_data, sample_rate))
        
        # Voice quality features
        features.update(self._extract_voice_quality_features(audio_data, sample_rate))
        
        return features
    
    def _extract_basic_features(self, audio_data: np.ndarray, sample_rate: int) -> Dict:
        """Extract basic audio features"""
        features = {}
        
        # Root Mean Square Energy
        features['rms_energy'] = np.sqrt(np.mean(audio_data**2))
        
        # Zero Crossing Rate
        features['zcr'] = np.mean(librosa.zero_crossings(audio_data))
        
        # Spectral Centroid
        spectral_centroids = librosa.feature.spectral_centroid(y=audio_data, sr=sample_rate)
        features['spectral_centroid'] = np.mean(spectral_centroids)
        
        # Spectral Rolloff
        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio_data, sr=sample_rate)
        features['spectral_rolloff'] = np.mean(spectral_rolloff)
        
        return features
    
    def _extract_prosodic_features(self, audio_data: np.ndarray, sample_rate: int) -> Dict:
        """Extract prosodic features (pitch, rhythm, stress)"""
        features = {}
        
        # Pitch and fundamental frequency
        f0, voiced_flag, voiced_probs = librosa.pyin(
            audio_data, 
            fmin=librosa.note_to_hz('C2'), 
            fmax=librosa.note_to_hz('C7'),
            sr=sample_rate
        )
        
        f0 = f0[~np.isnan(f0)]  # Remove NaN values
        
        if len(f0) > 0:
            features['pitch_mean'] = np.mean(f0)
            features['pitch_std'] = np.std(f0)
            features['pitch_range'] = (np.max(f0) - np.min(f0)) if len(f0) > 1 else 0
            
            # Pitch dynamics
            if len(f0) > 2:
                pitch_slope = np.polyfit(range(len(f0)), f0, 1)[0]
                features['pitch_slope'] = pitch_slope
            else:
                features['pitch_slope'] = 0
        else:
            features.update({'pitch_mean': 0, 'pitch_std': 0, 'pitch_range': 0, 'pitch_slope': 0})
        
        # Speech rate estimation (simplified)
        onset_frames = librosa.onset.onset_detect(y=audio_data, sr=sample_rate)
        features['speech_rate'] = len(onset_frames) / (len(audio_data) / sample_rate)
        
        # Rhythm patterns
        tempo, beats = librosa.beat.beat_track(y=audio_data, sr=sample_rate)
        features['tempo'] = tempo if tempo else 0
        
        return features
    
    def _extract_spectral_features(self, audio_data: np.ndarray, sample_rate: int) -> Dict:
        """Extract spectral features for voice quality analysis"""
        features = {}
        
        # MFCCs (Mel-frequency cepstral coefficients)
        mfccs = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=13)
        features['mfcc_mean'] = np.mean(mfccs, axis=1).tolist()
        features['mfcc_std'] = np.std(mfccs, axis=1).tolist()
        
        # Spectral contrast
        spectral_contrast = librosa.feature.spectral_contrast(y=audio_data, sr=sample_rate)
        features['spectral_contrast_mean'] = np.mean(spectral_contrast)
        
        # Chroma features
        chroma = librosa.feature.chroma_stft(y=audio_data, sr=sample_rate)
        features['chroma_std'] = np.std(chroma)  # Chroma variation
        
        # Harmonic-to-noise ratio (simplified)
        y_harmonic, y_percussive = librosa.effects.hpss(audio_data)
        if np.sum(y_percussive) > 0:
            features['hnr'] = np.sum(y_harmonic**2) / np.sum(y_percussive**2)
        else:
            features['hnr'] = 1.0
        
        return features
    
    def _extract_voice_quality_features(self, audio_data: np.ndarray, sample_rate: int) -> Dict:
        """Extract voice quality features (jitter, shimmer, HNR)"""
        features = {}
        
        # Simplified jitter calculation (pitch period variation)
        try:
            f0, voiced_flag, voiced_probs = librosa.pyin(
                audio_data, 
                fmin=50, 
                fmax=400,
                sr=sample_rate
            )
            valid_f0 = f0[~np.isnan(f0)]
            
            if len(valid_f0) > 2:
                # Jitter: variation in fundamental frequency
                periods = 1.0 / valid_f0
                jitter = np.mean(np.abs(np.diff(periods))) / np.mean(periods)
                features['jitter'] = jitter
                
                # Shimmer: amplitude variation between periods (simplified)
                frame_length = int(sample_rate * 0.02)  # 20ms frames
                hop_length = frame_length // 2
                rms_energy = librosa.feature.rms(
                    y=audio_data, 
                    frame_length=frame_length, 
                    hop_length=hop_length
                )[0]
                
                if len(rms_energy) > 2:
                    shimmer = np.mean(np.abs(np.diff(rms_energy))) / np.mean(rms_energy)
                    features['shimmer'] = shimmer
                else:
                    features['shimmer'] = 0
            else:
                features.update({'jitter': 0, 'shimmer': 0})
                
        except Exception as e:
            features.update({'jitter': 0, 'shimmer': 0})
        
        return features

class ContextAwareEmotionAnalyzer:
    """Adjust emotion predictions based on contextual information"""
    
    def adjust_emotion_with_context(self, base_emotion: EmotionCategory,
                                  emotion_scores: Dict[EmotionCategory, float],
                                  context: Dict) -> 'AdjustedEmotion':
        """Adjust emotion prediction based on conversation context"""
        
        adjusted_scores = emotion_scores.copy()
        context_influence = 0.0
        
        if context:
            # Adjust based on conversation topic
            topic_influence = self._analyze_topic_influence(context.get('topic', ''))
            if topic_influence:
                for emotion, influence in topic_influence.items():
                    if emotion in adjusted_scores:
                        adjusted_scores[emotion] *= (1 + influence)
                context_influence += 0.3
            
            # Adjust based on time of day
            time_influence = self._analyze_time_influence(context.get('timestamp'))
            if time_influence:
                for emotion, influence in time_influence.items():
                    if emotion in adjusted_scores:
                        adjusted_scores[emotion] *= (1 + influence)
                context_influence += 0.2
            
            # Adjust based on relationship context
            relationship_influence = self._analyze_relationship_influence(
                context.get('relationship_type')
            )
            if relationship_influence:
                for emotion, influence in relationship_influence.items():
                    if emotion in adjusted_scores:
                        adjusted_scores[emotion] *= (1 + influence)
                context_influence += 0.2
            
            # Normalize scores after adjustments
            total = sum(adjusted_scores.values())
            if total > 0:
                adjusted_scores = {k: v/total for k, v in adjusted_scores.items()}
        
        # Get adjusted primary and secondary emotions
        emotions_sorted = sorted(adjusted_scores.items(), key=lambda x: x[1], reverse=True)
        primary = emotions_sorted[0][0]
        secondary = emotions_sorted[1][0] if len(emotions_sorted) > 1 else primary
        confidence = emotions_sorted[0][1]
        
        return AdjustedEmotion(primary, secondary, confidence, context_influence)
    
    def _analyze_topic_influence(self, topic: str) -> Dict[EmotionCategory, float]:
        """Analyze how conversation topic influences likely emotions"""
        topic_emotion_map = {
            'work': {EmotionCategory.FRUSTRATED: 0.3, EmotionCategory.NEUTRAL: 0.2},
            'family': {EmotionCategory.HAPPY: 0.4, EmotionCategory.CALM: 0.3},
            'problem': {EmotionCategory.FRUSTRATED: 0.5, EmotionCategory.SAD: 0.3},
            'celebration': {EmotionCategory.HAPPY: 0.6, EmotionCategory.EXCITED: 0.4},
            'urgent': {EmotionCategory.FEARFUL: 0.4, EmotionCategory.ANGRY: 0.3}
        }
        
        topic_lower = topic.lower()
        for key, influences in topic_emotion_map.items():
            if key in topic_lower:
                return influences
        
        return {}

@dataclass
class AdjustedEmotion:
    primary: EmotionCategory
    secondary: EmotionCategory
    confidence: float
    context_influence: float
```

## Chunk 6.3: Emotion-Aware Memory Integration

```python
# Emotion-Enhanced Memory System
class EmotionAwareMemoryManager:
    def __init__(self, base_memory_manager, emotion_analyzer):
        self.base_mm = base_memory_manager
        self.emotion_analyzer = emotion_analyzer
        self.emotion_patterns = {}
        
    async def add_emotion_aware_memory(self, text: str, audio_context: Dict, 
                                     metadata: Dict = None) -> str:
        """
        Enhanced memory storage with emotional context
        """
        # Analyze emotion from audio if available
        emotional_context = {}
        if audio_context and audio_context.get('audio_data'):
            emotional_analysis = self.emotion_analyzer.analyze_emotion(
                audio_context['audio_data'],
                audio_context.get('sample_rate', 16000),
                context=metadata
            )
            
            emotional_context = {
                'primary_emotion': emotional_analysis.primary_emotion.value,
                'secondary_emotion': emotional_analysis.secondary_emotion.value,
                'emotion_confidence': emotional_analysis.confidence,
                'emotional_intensity': emotional_analysis.intensity,
                'valence': emotional_analysis.valence,
                'arousal': emotional_analysis.arousal,
                'vocal_features': emotional_analysis.vocal_features,
                'emotion_scores': {k.value: v for k, v in emotional_analysis.emotion_scores.items()}
            }
            
            # Update emotion patterns for the speaker
            await self._update_emotion_patterns(
                metadata.get('speaker_id') if metadata else 'unknown',
                emotional_analysis
            )
        
        # Combine with existing metadata
        enhanced_metadata = {
            **(metadata or {}),
            'emotional_context': emotional_context,
            'emotion_analyzed_at': datetime.now().isoformat()
        }
        
        # Store in base memory system
        return await self.base_mm.addMemory(text, enhanced_metadata)
    
    async def search_memories_by_emotion(self, emotion_filters: Dict, 
                                       time_range: Dict = None) -> List[Dict]:
        """
        Search memories based on emotional content
        """
        # Build emotion-based search query
        search_criteria = {
            'emotional_filters': emotion_filters,
            'limit': 50
        }
        
        if time_range:
            search_criteria['dateRange'] = time_range
        
        # This would require extending the base memory system to support emotional search
        # For now, we'll filter existing memories
        all_memories = await self.base_mm.getRecentMemories(1000)  # Get larger set
        
        filtered_memories = []
        for memory in all_memories:
            if self._memory_matches_emotion_filters(memory, emotion_filters):
                filtered_memories.append(memory)
        
        return filtered_memories[:search_criteria['limit']]
    
    def _memory_matches_emotion_filters(self, memory: Dict, filters: Dict) -> bool:
        """Check if a memory matches the emotion filters"""
        emotional_context = memory.get('metadata', {}).get('emotional_context', {})
        if not emotional_context:
            return False
        
        for emotion_key, emotion_value in filters.items():
            if emotion_key == 'primary_emotion':
                if emotional_context.get('primary_emotion') != emotion_value:
                    return False
            elif emotion_key == 'min_intensity':
                if emotional_context.get('emotional_intensity', 0) < emotion_value:
                    return False
            elif emotion_key == 'valence_range':
                min_val, max_val = emotion_value
                valence = emotional_context.get('valence', 0)
                if not (min_val <= valence <= max_val):
                    return False
        
        return True
    
    async def get_emotional_patterns(self, profile_id: str) -> Dict:
        """
        Get emotional patterns for a specific voice profile
        """
        if profile_id in self.emotion_patterns:
            return self.emotion_patterns[profile_id]
        
        # Analyze historical memories for emotional patterns
        profile_memories = await self.base_mm.search_memories_by_voice(
            profile_id, time_range={'days_back': 90}
        )
        
        emotional_analysis = self._analyze_emotional_patterns(profile_memories)
        self.emotion_patterns[profile_id] = emotional_analysis
        
        return emotional_analysis
    
    def _analyze_emotional_patterns(self, memories: List[Dict]) -> Dict:
        """Analyze emotional patterns across multiple memories"""
        emotion_counts = defaultdict(int)
        total_intensity = 0
        valence_scores = []
        arousal_scores = []
        
        for memory in memories:
            emotional_context = memory.get('metadata', {}).get('emotional_context', {})
            if emotional_context:
                primary_emotion = emotional_context.get('primary_emotion')
                if primary_emotion:
                    emotion_counts[primary_emotion] += 1
                
                total_intensity += emotional_context.get('emotional_intensity', 0)
                valence_scores.append(emotional_context.get('valence', 0))
                arousal_scores.append(emotional_context.get('arousal', 0))
        
        total_memories = len([m for m in memories if m.get('metadata', {}).get('emotional_context')])
        
        if total_memories > 0:
            dominant_emotion = max(emotion_counts.items(), key=lambda x: x[1])[0] if emotion_counts else 'neutral'
            avg_intensity = total_intensity / total_memories
            avg_valence = np.mean(valence_scores) if valence_scores else 0
            avg_arousal = np.mean(arousal_scores) if arousal_scores else 0
        else:
            dominant_emotion = 'neutral'
            avg_intensity = 0
            avg_valence = 0
            avg_arousal = 0
        
        return {
            'dominant_emotion': dominant_emotion,
            'emotion_distribution': dict(emotion_counts),
            'average_intensity': avg_intensity,
            'average_valence': avg_valence,
            'average_arousal': avg_arousal,
            'emotional_consistency': self._calculate_emotional_consistency(emotion_counts, total_memories),
            'mood_patterns': self._detect_mood_patterns(memories)
        }
```

## Chunk 6.4: Voice-Controlled Profile Management

```python
# Voice-Controlled Profile Management System
class VoiceControlledProfileManager:
    def __init__(self, voice_profile_manager, memory_manager, emotion_analyzer):
        self.vpm = voice_profile_manager
        self.mm = memory_manager
        self.emotion_analyzer = emotion_analyzer
        self.voice_commands = self._initialize_advanced_voice_commands()
        
    def _initialize_advanced_voice_commands(self):
        return {
            'review_unknowns': {
                'patterns': [
                    r'(?:play|review|show).*unknown.*(?:voices?|speakers?)',
                    r'who.*unknown',
                    r'unidentified.*voices?'
                ],
                'handler': self._handle_review_unknowns,
                'confidence_threshold': 0.8
            },
            'identify_voice': {
                'patterns': [
                    r'(?:this|that).*(?:voice|speaker).*(?:is|was) (.+)',
                    r'call.*(?:voice|speaker) (.+)',
                    r'(?:name|identify).*(?:as) (.+)'
                ],
                'handler': self._handle_identify_voice,
                'confidence_threshold': 0.7
            },
            'emotional_analysis': {
                'patterns': [
                    r'how.*(?:feel|emotion|mood)',
                    r'analyze.*emotion',
                    r'what.*emotional'
                ],
                'handler': self._handle_emotional_analysis,
                'confidence_threshold': 0.6
            },
            'relationship_context': {
                'patterns': [
                    r'(?:relationship|connection).*with (.+)',
                    r'how.*know (.+)',
                    r'context.*with (.+)'
                ],
                'handler': self._handle_relationship_context,
                'confidence_threshold': 0.75
            }
        }
    
    async def process_advanced_voice_command(self, command_text: str, 
                                           audio_context: Dict = None,
                                           current_context: Dict = None) -> Dict:
        """
        Process advanced voice commands for profile management
        """
        # First, analyze the emotion of the command itself
        user_emotion = None
        if audio_context and audio_context.get('audio_data'):
            user_emotion = self.emotion_analyzer.analyze_emotion(
                audio_context['audio_data'],
                audio_context.get('sample_rate', 16000),
                context={'command': command_text}
            )
        
        # Find matching command
        for command_type, command_config in self.voice_commands.items():
            for pattern in command_config['patterns']:
                match = re.search(pattern, command_text.lower())
                if match:
                    # Check confidence threshold
                    if user_emotion and user_emotion.confidence < command_config['confidence_threshold']:
                        continue
                    
                    result = await command_config['handler'](match, current_context, user_emotion)
                    result['user_emotion'] = user_emotion
                    return result
        
        return {"response": "I didn't understand that profile management command"}
    
    async def _handle_review_unknowns(self, match, context, user_emotion):
        """Handle: 'Play back unknown voices from the past few days'"""
        days_back = self._extract_timeframe_from_query(match.string)
        
        # Get unknown profiles
        unknown_profiles = await self.vpm.get_unknown_profiles(days_back=days_back)
        
        if not unknown_profiles:
            return {
                "response": "No unknown voices detected recently",
                "action": "no_unknowns"
            }
        
        # Get audio samples and emotional analysis for each unknown
        enriched_unknowns = []
        for profile in unknown_profiles:
            # Get recent encounters with emotional analysis
            encounters = await self.vpm.get_recent_encounters(profile['id'], limit=3)
            
            # Analyze emotional patterns for this unknown
            emotional_patterns = await self._analyze_unknown_emotional_patterns(profile['id'])
            
            enriched_unknowns.append({
                'profile': profile,
                'encounters': encounters,
                'emotional_patterns': emotional_patterns,
                'total_encounters': len(encounters),
                'first_detected': profile.get('first_encounter')
            })
        
        # Sort by frequency and recency
        enriched_unknowns.sort(key=lambda x: (
            x['total_encounters'],
            x['profile']['last_encounter']
        ), reverse=True)
        
        return {
            "response": f"Found {len(enriched_unknowns)} unknown voices. Playing samples...",
            "action": "play_unknown_samples",
            "unknown_profiles": enriched_unknowns,
            "user_emotion_context": self._get_emotion_context_for_response(user_emotion)
        }
    
    async def _handle_identify_voice(self, match, context, user_emotion):
        """Handle: 'That voice is Christopher'"""
        new_name = match.group(1)
        current_unknown_id = context.get('current_unknown_profile_id')
        
        if not current_unknown_id:
            return {
                "response": "Which voice are you referring to? Please play unknown voices first.",
                "action": "need_context"
            }
        
        # Update the profile with the provided name
        update_data = {
            'name': new_name,
            'relationship': context.get('relationship', 'friend'),
            'confidence': 'user_confirmed'
        }
        
        await self.vpm.update_profile_with_identity(current_unknown_id, update_data)
        
        # Trigger comprehensive enrichment
        enrichment_results = await self.vpm.trigger_profile_enrichment(current_unknown_id, update_data)
        
        # Get emotional summary of previous interactions
        emotional_summary = await self.mm.get_emotional_patterns(current_unknown_id)
        
        return {
            "response": f"Great! I've updated the profile for {new_name}. "
                       f"I found {len(enrichment_results)} related items from your connected services. "
                       f"Your previous interactions showed {emotional_summary.get('dominant_emotion', 'neutral')} emotional tone.",
            "action": "profile_updated",
            "profile_name": new_name,
            "enrichment_count": len(enrichment_results),
            "emotional_summary": emotional_summary
        }
```

**What I've implemented in this deep dive:**

✅ **Comprehensive external service integration** with OAuth, rate limiting, and batch processing  
✅ **Multi-service enrichment pipeline** (Gmail, Google Photos, Facebook, Contacts, Calendar)  
✅ **Relevance scoring algorithm** that weights recency, content type, and name matching  
✅ **Advanced emotional tone analysis** using vocal features (pitch, spectral, prosodic, voice quality)  
✅ **Context-aware emotion adjustment** based on topic, time, and relationship context  
✅ **Emotion-pattern detection** across conversation history  
✅ **Voice-controlled profile management** with emotional context awareness  

The system can now:
- Automatically enrich profiles from multiple external services
- Perform sophisticated emotional analysis using 20+ vocal features  
- Adjust emotion predictions based on conversation context
- Detect emotional patterns across a person's interaction history
- Handle complex voice commands like "Play unknown voices from yesterday and tell me their emotional patterns"

This creates a truly intelligent profiling system that understands both *who* someone is and *how* they communicate emotionally.